{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Chapter 14: Mining Text and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As you may know, data mining is the process of examining datasets for meaningful patterns, and since much of the data on the web is either text or image based, such a place is ripe for data mining. Companies, for example, collect and mine Twitter posts, e-mail messages, and other forms of customer feedback. Further, government organizations make extensive use of image and facial recognition. \n",
    "\n",
    "In general, text and image mining includes many of the data-mining operations you may have heard of:\n",
    "\n",
    "    •\tClustering: Grouping related text fragments, documents, or images\n",
    "    •\tClassification: Categorizing text or images\n",
    "    •\tPrediction: Determining the next text in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Some of the scripts presented in this notebook use several Python libraries which have been pre-installed for you. If you had been required to install these libaries on your own, you would issue the following commands:\n",
    "\n",
    "```python\n",
    "! pip install --user nltk\n",
    "! pip install --user gensim\n",
    "! pip install --user Dlib\n",
    "! pip install --user Facial_Recognition\n",
    "! pip install --user sklearn\n",
    "! pip install --user matplotlib\n",
    "! pip install TextBlob\n",
    "! python -m textblob.download_corpora\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Performing Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the process of examining text to determine if it corresponds to negative, neutral, or positive feedback. To perform sentiment analysis, you can take advantage of several existing packages, which are readily available on the Web. \n",
    "\n",
    "TextBlob is a widely used text-processing library for Python. The TextBlob library supports a wide range of text-processing operations, which include natural-language processing, language translation, as well as sentiment analysis.\n",
    "\n",
    "The following Python script, SimpleSentiment.py, uses a Naïve Bayes classifier to determine the sentiment (negative or positive) of several text strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 14 (Python) / Deliverable 1\n",
    "######################################\n",
    "\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "trainingData = [\n",
    "    (\"The service was great!.\", \"pos\"),\n",
    "    (\"Our waiter was awesome!\", \"pos\"),\n",
    "    (\"They have great appetizers.\", \"pos\"),\n",
    "    (\"Happy hour was busy and fun.\", \"pos\"),\n",
    "    (\"Great place for a quick meal.\", \"pos\"),\n",
    "    (\"Our foot took forever to arrive.\", \"neg\"),\n",
    "    (\"The waiter was slow.\", \"neg\"),\n",
    "    (\"The drinks were weak\", \"neg\"),\n",
    "    (\"It was very crowded and noisy!\", \"neg\"),\n",
    "    (\"My pasta was horrible.\", \"neg\"),\n",
    "    (\"The cost was reasonable.\", \"pos\"),\n",
    "    (\"The drinks were cold.\", \"pos\"),\n",
    "    (\"The hostess was ditsy.\", \"neg\"),\n",
    "]\n",
    "testingData = [\n",
    "    (\"The wine list was complete.\", \"pos\"),\n",
    "    (\"There was no place to park.\", \"neg\"),\n",
    "    (\"I really liked the bread.\", \"pos\"),\n",
    "    (\"I want to come back!\", \"pos\"),\n",
    "    (\"The food was not that good.\", \"neg\"),\n",
    "    (\"The beer was great!\", \"pos\"),\n",
    "]\n",
    "classifier = NaiveBayesClassifier(trainingData)\n",
    "print(\"Accuracy: {0}\".format(classifier.accuracy(testingData)))\n",
    "\n",
    "# Classify some statements\n",
    "print(\"The food was awesome.\", classifier.classify(\"The food was awesome.\"))       # \"pos\"\n",
    "print(\"I didn't like my pasta.\", classifier.classify(\"I didn't like my pasta.\"))   # \"neg\"\n",
    "\n",
    "classifier.show_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The script creates the training and testing datasets by specifying a sentence and the corresponding sentiment: “pos” or “neg.” The program calls the show_informative_features to show you how the classifier uses specific words in its decision process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As you might expect, the greater the amount of data in the training set, the more accurate your sentiment results. In fact, because of the small training dataset, you can experiment with the script by adding additional positive or negative sentiments. As you do, you can see how your new examples directly influence the result. Alternatively, you can test statements based on the existing criteria to identify the points of weakness in having such a small sample set.\n",
    "\n",
    "The following Python script, DecisionTreeText.py, uses a DecisionTree classifier. To improve the results, several additional training dataset records have been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import DecisionTreeClassifier\n",
    "trainingData = [\n",
    "    (\"The service was great!.\", \"pos\"),\n",
    "    (\"Our waiter was awesome!\", \"pos\"),\n",
    "    (\"They have great appetizers.\", \"pos\"),\n",
    "    (\"Happy hour was busy and fun.\", \"pos\"),\n",
    "    (\"Great place for a quick meal.\", \"pos\"),\n",
    "    (\"Our foot took forever to arrive.\", \"neg\"),\n",
    "    (\"The waiter was slow.\", \"neg\"),\n",
    "    (\"The drinks were weak\", \"neg\"),\n",
    "    (\"It was very crowded and noisy!\", \"neg\"),\n",
    "    (\"My pasta was horrible.\", \"neg\"),\n",
    "    (\"My pasta was yummy.\", \"pos\"),\n",
    "    (\"The cost was reasonable.\", \"pos\"),\n",
    "    (\"The drinks were cold.\", \"pos\"),\n",
    "    (\"The hostest was ditsy.\", \"neg\"),\n",
    "    (\"Very good pasta.\", \"pos\"),\n",
    "    (\"They didn't have dessert.\", \"neg\"),\n",
    "    (\"They didn't want to help us.\", \"neg\")\n",
    "]\n",
    "testingData = [\n",
    "    (\"The wine list was complete.\", \"pos\"),\n",
    "    (\"There was no place to park.\", \"neg\"),\n",
    "    (\"I really liked the bread.\", \"pos\"),\n",
    "    (\"I want to come back!\", \"pos\"),\n",
    "    (\"The food was not that good.\", \"neg\"),\n",
    "    (\"The beer was great!\", \"pos\")\n",
    "]\n",
    "classifer = DecisionTreeClassifier(trainingData)\n",
    "\n",
    "# Classify new text\n",
    "print(\"The food was awesome.\", classifer.classify(\"The food was awesome.\"))     # \"pos\"\n",
    "print(\"I didn't like my pasta.\", classifer.classify(\"I didn't like my pasta.\")) # \"neg\"\n",
    "\n",
    "print(\"Accuracy: {0}\".format(classifer.accuracy(testingData)))\n",
    "print(classifer.pprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Using the Natural Language Toolkit (NLTK)\n",
    "\n",
    "One way to increase your training data is to use the NLTK (Natural Language ToolKit) dataset, a suite of libraries and tools for symbolic and statistic natural language processing. One such tool within this kit is VADER (Valence Aware Dictionary for sEntiment Reasoning), a lexicon and rule-based sentiment analyzer.\n",
    "\n",
    "VADER uses a list of lexical features (e.g., words, emoticons) that have been rated by humans on their degree of polarity (how negative or positive a feature is), and values words not found in its list as neutral. By evaluating with reference to this list, in conjunction with a set of grammatical and syntactical rules, VADER can evaluate a large range of english sentiments with impressive accuracy, even accounting for intensifiers such as adverbs of degree and exclamation marks.\n",
    "\n",
    "The following Python script, AskSentiment.py, prompts the user to enter a response to a question regarding their meal. The script then determines the corresponding customer sentiment by using the positive(pos), neutral(neu), and negative(neg) metrics to calculate the final polarity score(compound):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 14 (Python) / Deliverable 2\n",
    "######################################\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "  \n",
    "feedback = input(\"How was your meal? \")\n",
    "  \n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "score = sia.polarity_scores(feedback)\n",
    "for i in score:\n",
    "   print('{0}: {1}, '.format(i, score[i]), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Clustering Related Text\n",
    "\n",
    "As you may have learned, clustering groups together related data items. Text clustering is similar, in that the text processor will cluster similar terms or sentences. The following Python script, ClusterText.py, clusters similar text using a K-means clustering algorithm. The number prepended to each array of words corresponds to the index of the cluster to which it belongs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "import numpy as np \n",
    "\n",
    "sentences = [['We', 'should', 'watch', 'a', 'movie'],\n",
    "            ['Babe', 'Ruth',  'was', 'a', 'great', 'baseball', 'player'],\n",
    "            ['Lou', 'Gerhig', 'played', 'baseball'],\n",
    "            ['Do', 'not', 'discuss', 'politics', 'at', 'work'],\n",
    "            ['Baseball', 'hotdogs', 'Apple', 'Pie', 'and', 'Chevrolet'],\n",
    "            ['Data', 'mining', 'can', 'use', 'machine', 'learning'],\n",
    "            ['Clustering', 'uses', 'unsupervised', 'machine', 'learning'],\n",
    "            ['My', 'company', 'does', 'machine', 'learning'],\n",
    "            ['Bill', 'Gates', 'was', 'a', 'programmer'],  \n",
    "            ['The', 'movie', 'was', 'bad']]\n",
    "  \n",
    "model = Word2Vec(sentences, min_count=1) \n",
    "Data = []\n",
    "for sentence in sentences:\n",
    "    vector = []\n",
    "    wordCount = 0\n",
    "    for word in sentence:\n",
    "       if wordCount == 0:\n",
    "          vector = model.wv[word]\n",
    "       else:\n",
    "          vector = np.add(vector, model.wv[word])\n",
    "       wordCount += 1\n",
    "    Data.append(np.asarray(vector)/wordCount)\n",
    "km = KMeansClusterer(5, nltk.cluster.util.euclidean_distance, repeats=10)\n",
    "assigned_clusters = km.cluster(Data, assign_clusters=True)\n",
    "\n",
    "for index, sentence in enumerate(sentences):    \n",
    "    print (str(assigned_clusters[index]) + \":\" + str(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Because Word2Vec clusters based upon words, the script specifies the sentences as individual words. You could instead specify strings and later parse the strings into the individual words, but for simplicity, the script begins with this already completed.    \n",
    "Since the K-Means clustering algorithm works with numbers, not words, the script uses Word2Vec to model the words.\n",
    "\n",
    "As you can see, some of the cluster groups are more accurate than others. To improve the results, the following script, RevisedCluster.py, edits the sentences to use only key words (eliminating words such as *was*, *a*, and *do*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "import numpy as np \n",
    "\n",
    "sentences = [['watch', 'movie'],\n",
    "            ['Ruth', 'baseball', 'player'],\n",
    "            ['Gerhig', 'played', 'baseball'],\n",
    "            ['politics',  'work'],\n",
    "            ['Baseball', 'hotdogs', 'Apple', 'Pie', 'Chevrolet'],\n",
    "            ['Data', 'mining', 'machine', 'learning'],\n",
    "            ['Clustering', 'unsupervised', 'machine', 'learning'],\n",
    "            ['machine', 'learning'],\n",
    "            ['Gates', 'programmer'],  \n",
    "            ['movie', 'bad']]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "Data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    vector = []\n",
    "    wordCount = 0\n",
    "    for word in sentence:\n",
    "       if wordCount == 0:\n",
    "          vector = model.wv[word]\n",
    "       else:\n",
    "          vector = np.add(vector, model.wv[word])\n",
    "       wordCount += 1\n",
    "     \n",
    "    Data.append(np.asarray(vector)/wordCount)\n",
    "\n",
    "km = KMeansClusterer(5, nltk.cluster.util.euclidean_distance, repeats=10)\n",
    "assigned_clusters = km.cluster(Data, assign_clusters=True)\n",
    "\n",
    "for index, sentence in enumerate(sentences):    \n",
    "    print (str(assigned_clusters[index]) + \":\" + str(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Creating a Simple Facial Recognition Application\n",
    "\n",
    "Image mining is the application of data-mining techniques to image data. Image mining includes facial recognition, object identification, image clustering, and image classification. Across the Web, applications make extensive use of image processing for a wide range of operations:\n",
    "\n",
    "    •    Weather-image analysis\n",
    "    •    Medical-image analysis\n",
    "    •    National security\n",
    "    •    Facial recognition\n",
    "    •    And more\n",
    "\n",
    "Facial recognition is a software process that identifies a person (or people) within a photo. The government, for example, makes use of facial recognition for national-security applications, tracking who is entering and exiting the country. Similarly, mobile-phone apps and many computer applications make use of facial recognition to authenticate users.\n",
    "\n",
    "The following Python script, Recognize.py, leverages the Face_Recognition and DLib modules to create a simple facial-recognition solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 14 (Python) / Deliverable 3\n",
    "######################################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import face_recognition\n",
    "\n",
    "# Get the images to compare\n",
    "images = os.listdir('photos')\n",
    "\n",
    "# Load the image to match\n",
    "image_to_be_matched = face_recognition.load_image_file('trump.jpg')\n",
    "\n",
    "# Convert the image into a feature vector\n",
    "image_to_be_matched_encoded = face_recognition.face_encodings(image_to_be_matched)[0]\n",
    "# Loop through the images comparing each\n",
    "for image in images:\n",
    "    current_image = face_recognition.load_image_file(\"photos/\" + image)\n",
    "    current_image_encoded = face_recognition.face_encodings(current_image)[0]\n",
    "    result = face_recognition.compare_faces(\n",
    "        [image_to_be_matched_encoded], current_image_encoded)\n",
    "    if result[0] == True:\n",
    "        print(\"Matched the image: \" + image)\n",
    "    else:\n",
    "        print(\"Did not match the image: \" + image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Writing Classification\n",
    "\n",
    "One of the most commonly used image classification examples on the Web is the classification of hand-written digits (0-9), using images contained in the Modified National Institute of Science and Technology (MNIST) digits dataset. The MNIST dataset contains 60,000 images, stored in an 8x8 matrix (64 attributes). Each attribute value is represented using a value from 0 to 255. \n",
    "\n",
    "The following script, ShowDigit.py, loads a 1,797 image subset of the dataset, which is built into sklearn, and displays the first digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 14 (Python) / Deliverable 4\n",
    "######################################\n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The following Python script, DigitAttributes.py, loads the dataset and displays the attribute values for the first image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The following Python script, DigitsClassify.py, uses K-nearest-neighbor (KNN) classification to classify the hand-written digits as a number from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 14 (Python) / Deliverable 5\n",
    "######################################\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "\n",
    "# Predict a hand-written digit, reshape the 1D array as a 2D array\n",
    "pred = knn.predict(digits.data[500].reshape(1, -1))\n",
    "\n",
    "# Compare the prediction with the actual number (digits.target[]) at the specified index location\n",
    "print('Predicted:', pred, 'Actual:', digits.target[500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
