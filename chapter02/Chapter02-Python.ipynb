{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Chapter 2: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Some of the scripts presented in this notebook use several Python libraries which have been pre-installed for you. If you had been required to install these libaries on your own, you would issue the following commands:\n",
    "\n",
    "```python\n",
    "! pip install --user pandas\n",
    "! pip install --user numpy\n",
    "! pip install --user sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Dataframe\n",
    "\n",
    "**Note: This notebook is intended to provide a demonstration of common machine-learning operations. As such, you are not expected to understand the entirety of the code in each sample.*\n",
    "\n",
    "Supervised learning is the use of an algorithm that uses labeled data to produce a training dataset an algorithm can use to learn how to identify patterns. A solution that uses a supervised machine-learning approach, such as data classification, will perform the following steps:\n",
    "\n",
    "1. Specify a training dataset from which the model can learn to match patterns.\n",
    "2. Specify a testing dataset that the model can use to test its accuracy.\n",
    "3. Use the model with new data to classify data or predict results.\n",
    "\n",
    "Take the following Python script, TrainTest.py, for example, which performs the first 2 steps of the supervised machine-learning approach. This script reads the Breast.data.csv file, which contains attributes with which a machine-learning program can classify breast-cancer tumor data as malignant or benign, into a dataframe object. The script assigns 30% of the dataset to the testing dataframed the remainder (30%) goes to the training dataset. The script finishes by displaying the number of total rows, training rows, and testing rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 2 (Python) / Deliverable 1\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:9])\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "print(\"Total rows: \", len(df))\n",
    "print(\"Training rows: \", len(X_train))\n",
    "print(\"Testing rows: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Classifying E-mail as Spam\n",
    "\n",
    "To test a model, a supervised machine-learning program uses the test attributes to determine a result, which it then compares to the known result. As you continue your exploration of machine-learning solutions, you will find that developers will normally divide a given dataset into training and testing sets such that the training set contains 70-80% of the data, and the testing set the remainder. Assume, for example, your goal is to identify incoming email as valid or spam.\n",
    "\n",
    "The following Python script, ClassifySpam.py, uses the Spam.csv dataset to classify e-mails as valid or spam. In this case, the script uses the read_csv function to read the dataset file into a dataframe. The script then assigns the independent variables to the X array and the dependent values to the y array. Next, the script assigns the training and test datasets using the K-nearest-neighbors algorithm to classify the data. After classifying the data, the script displays its accuracy and confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 2 (Python) / Deliverable 2\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.read_csv('Spam.csv', header=None) # read dataset file into a dataframe\n",
    "X = np.array(df.iloc[:, 0:40]) \t# independent variables (attributes that have high correlation with an email's validity)\n",
    "y = np.array(df[57]) # dependent variable (valid or spam)\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "    \n",
    "print('\\nConfusion Matrix\\n', confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Clustering Stock Data\n",
    "\n",
    "Data clustering is the process of assigning data items into related groups. In contrast to supervised machine-learning, as demonstrated in the data classification exercise above, clustering uses unsupervised learning. This means that it does not have correctly labeled data from which training and testing datasets can be used. Consider the Dow Jones Stocks dataset, which contains stock prices (open, high, low and close) and trading volume for many stocks. \n",
    "\n",
    "The following Python script, ClusterStocks.py, loads the DowJones.csv file and then uses K-means clustering to group the stocks into three related clusters. However, this stock prices dataset contains many different attributes, which are not well suited for clustering or scatter plotting. The script, therefore, uses the PCA library to transform the values into a two-dimensional array, the values of which are representative of the original data and which can be clustered and graphed. Lastly, within the cluster chart, the script marks the center of each cluster (called the centroid) with a black X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 2 (Python) / Deliverable 3\n",
    "#####################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv('DowJones.csv')\n",
    "\n",
    "df = df.drop('stock', axis=1)\n",
    "df = df.drop('date', axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "# decompose dataset into 2 representative components, then project on a 2D array.\n",
    "pca = PCA(n_components=2).fit(df)\n",
    "data_2d = pca.transform(df)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(data_2d)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "for i in range(0, data_2d.shape[0]):\n",
    "  if kmeans.labels_[i] == 0:\n",
    "    plt.scatter(data_2d[i,0], data_2d[i,1], c='green') \n",
    "  elif kmeans.labels_[i] == 1:\n",
    "    plt.scatter(data_2d[i,0], data_2d[i,1], c='yellow') \n",
    "  elif kmeans.labels_[i] == 2:\n",
    "    plt.scatter(data_2d[i,0], data_2d[i,1], c='blue')\n",
    "\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X') #  mark center of each cluster\n",
    "plt.title(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Scaling Dataset Values\n",
    "\n",
    "Depending on your dataset, there may be times when different attributes have different underlying scales. For example, a quality attribute may be based on the values 1 to 5, whereas a satisfaction attribute, the values 1 to 10. To improve the results of your machine-learning and data-mining operations, you should align the attribute scales. \n",
    "\n",
    "The following Python script, Scale.py, uses the StandardScaler function to do just that. Behind the scenes, the StandardScaler function will scale column values such that values have a mean of 0 and a standard deviation of 1. The script scales the values of a dataframe, showing the value before and after scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Data = {\n",
    "\t'x': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "        'y': [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "  \n",
    "print('Original Dataset')\n",
    "print(df)\n",
    "\n",
    "sc = StandardScaler()  \n",
    "scaled = sc.fit_transform(df)  \n",
    "\n",
    "print('\\nScaled Dataset')\n",
    "print(scaled)\n",
    "print('\\nMean:', scaled.mean())\n",
    "print('Standard deviation:', scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Understanding Dimensionality Reduction\n",
    "\n",
    "As the size of datasets increase, so too does the time required to process the data, as well as the amount of RAM required to hold it. Depending on the dataset with which you are working, there will often be times when one or more of the independent variables within the dataset do not influence the dependent variable. In such cases, you can delete the corresponding columns—a process to which data analysts refer as dimensionality reduction. \n",
    "\n",
    "A simple way to determine the relationship between variables is to determine the correlation value between them. Variables with a correlation value near 1, have a high correlation—as the value of one variable increases or decreases, so too will the value of the second. Variables with a correlation value of -1, the variables have a strong negative correlation, meaning, if you increase the value of one variable, the value of the second will decrease proportionally. Variables with a correlation value near 0 have no correlation—therefore, increasing or decreasing one variable’s value will have no impact on the second.\n",
    "\n",
    "The following Python script, BreastCancerCorrelations.py, examines the correlation between the independent variables and the dependent variable class, which specifies the whether a tumor is malignant or benign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "data = pd.read_csv('breast.data.csv', names=names)\n",
    "\n",
    "for i in range(1,9):\n",
    "  print('Correlation ', names[i], 'and class', np.corrcoef(data[names[i]], data['class'])[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Understanding Primary Component Analysis\n",
    "\n",
    "To perform dataset-dimensionality reduction, analysts will often use a technique called Primary Component Analysis (PCA) to determine (and select) the key independent variables. PCA is an unsupervised machine-learning algorithm. The following Python script, PCA.py, illustrates the use of PCA. In this case, the script does not reduce the dataset, but rather, displays the variance-attribute value for each variable. \n",
    "\n",
    "The analysis below shows that the primary variables are clump thickness, uniformity of cell size and uniformity of cell shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 1:9])\n",
    "y = np.array(df['class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n",
    "\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "x_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)  \n",
    "variance = pca.explained_variance_ratio_ \n",
    "\n",
    "for i in range(0, 8):\n",
    "  print(names[i+1], variance[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Now we can compare tests using all attributes in the dataset to tests using only the primary variables PCA identified.\n",
    "\n",
    "The following Python script, BreastCancerPCA.py, uses the K-nearest-neighbor classification method to determine if a breast-cancer tumor is malignant or benign. The script first uses all the independent variables to determine a result and then uses the first three variables identified by PCA. By reducing the number of independent variables to the three primary components, the model’s accuracy changes only slightly. In this case, the dataset is small, but if the dataset were very large, you can save considerable processing time by performing such a reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 2 (Python) / Deliverable 4\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names) \n",
    "X = np.array(df.iloc[:, 1:9]) # selects all attributes in the dataset\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('Model accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "    \n",
    "X = np.array(df.iloc[:, 2:4]) # selects the 3 high-variance attributes that PCA identified\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('Model accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Linear Discriminant Analysis (LDA)\n",
    "\n",
    "To reduce the dimensionality of datasets, analysts often perform Primary Component Analysis, an unsupervised-machine-learning algorithm. However, like most machine-learning concepts, there are many algorithms (approaches) to performing dimensionality reduction. Linear Discriminant Analysis (LDA) is a second approach that uses supervised-machine learning. The following Python script, LDA.py, illustrates the use of LDA. Again, the script will use the breast-cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 1:9])\n",
    "y = np.array(df['class'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()  \n",
    "X_train = sc.fit_transform(X_train)  \n",
    "X_test = sc.transform(X_test)  \n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "lda = LinearDiscriminantAnalysis(n_components=None)\n",
    "\n",
    "X_train = lda.fit_transform(X_train, y_train)\n",
    "X_test = lda.transform(X_test)  \n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy\", accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Mapping Categorical Values\n",
    "\n",
    "As you perform machine-learning operations, you will find that many algorithms require that the dataset values are numeric. Unfortunately, many datasets contain text-based categorical data. Consider, for example, the Census dataset, that contains attributes a machine-learning algorithm can use to determine whether an individual will make less or more than $50,000 a year. \n",
    "\n",
    "One way to change the text values to numeric is to edit the dataset using Excel. You can also use a programming language such as Python or R to make the changes. At first glance, you might be tempted to perform simple changes, such as converting the text values ‘male’ and ‘female’ to the values 1 and 0 and values such as ‘black’, ‘white’, and ‘other’ to values such as 3, 2, and 1. Although such substitutions accomplish the goal of getting numeric values, it has the problem of introducing ordinal values which imply a numeric order that male is greater than female and black has a more significant value than white, that has a more significant value than other. \n",
    "\n",
    "To avoid such ordered implications, developers use a technique called hot encoding that rather than assigning ordinal numbers, instead assigns binary vector values. In the case of male and female, you might use the following vectors:\n",
    "\n",
    "    male\t  [1 0]\n",
    "\tfemale\t[0 1]\n",
    "\n",
    "Likewise, for black, white, and other, you would use:\n",
    "\n",
    "\tblack    [1 0 0]\n",
    "\twhite\t[0 0 1]\n",
    "\tother\t[0 1 0]\n",
    "    \n",
    "The following Python script, OneHotEncoding.py, illustrates the process to encode categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data = ['black', 'white', 'other']\n",
    "print(data)\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "encodedvalues = lb.fit_transform(data)\n",
    "print(encodedvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The following Python script, OneHotEncodeCensus.py, illustrates the use of hot encoding to update a dataframe that contains the census data. The script starts by loading the adult.csv dataset into a dataframe and then using the head function to display the first five rows. The script then encodes the race values, creating the corresponding binary vectors. The script then appends those vectors as individual columns within the dataset. The script repeats this processing for the gender and country columns. The script then deletes the original (categorical) columns and displays the dataset’s contents. Before you perform your machine-learning and data-mining operations, you would repeat the steps for the remaining categorical fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('adult.csv')\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "encodedvalues = lb.fit_transform(df.iloc[:,8:9])\n",
    "print('Race')\n",
    "print(encodedvalues)\n",
    "\n",
    "dfOneHot = pd.DataFrame(encodedvalues, columns = [\"A\"+str(int(i)) for i in range(encodedvalues.shape[1])])\n",
    "df = pd.concat([df, dfOneHot], axis=1)\n",
    "\n",
    "encodedvalues = lb.fit_transform(df.iloc[:,9:10])\n",
    "print('Gender')\n",
    "print(encodedvalues)\n",
    "\n",
    "dfOneHot = pd.DataFrame(encodedvalues, columns = [\"B\"+str(int(i)) for i in range(encodedvalues.shape[1])])\n",
    "df = pd.concat([df, dfOneHot], axis=1)\n",
    "\n",
    "encodedvalues = lb.fit_transform(df.iloc[:,13:14])\n",
    "print('Country')\n",
    "print(encodedvalues)\n",
    "\n",
    "dfOneHot = pd.DataFrame(encodedvalues, columns = [\"C\"+str(int(i)) for i in range(encodedvalues.shape[1])])\n",
    "df = pd.concat([df, dfOneHot], axis=1)\n",
    "\n",
    "# delete the categorical column just replaced\n",
    "df.drop(df.columns[13], axis=1)\n",
    "df.drop(df.columns[8], axis=1)\n",
    "df.drop(df.columns[9], axis=1)\n",
    "\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
