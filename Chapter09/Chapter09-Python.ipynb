{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Chapter 9: Data Cleansing\n",
    "\n",
    "Data cleansing is the process of detecting, correcting, and removing errors and inconsistences from data. Such errors may be the result of bad user input, which may include incorrect spelling, numeric entry errors, inconsistent abbreviation of names and addresses, and so on. Likewise, some errors may occur due to a faulty sensor on an Internet of Things (IoT) device or a noisy data transmission line. Regardless of the cause of the error, the goal of data cleansing is higher quality data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "This notebook performs operations on a database which has been pre-built for you. The commands used to build this database can be found in the chapter09-database.sql script file located in the chapter folder.\n",
    "\n",
    "*Note: None of the MySQL statements found in this notebook are valid in the Jupyter environment. You are to run the code in the MySQL Workbench instance you prepared in *Getting Started with MySQL Workbench.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Testing for Non-Existent Records, Fields, and Null Values\n",
    "\n",
    "Completeness is a measure of the degree to which the data represents all required values. To measure the completeness of your data, you can determine first what percentage of the expected data values are present within your data set. For example, if a process collects a sensor’s values every second, for each hour of data, you should have 3,600 records:\n",
    "```python\n",
    "(60 records/minute)*(60 minutes/hour) = 3,600 records/hour\n",
    "```\n",
    "For simplicity, assume you record and store one minute of data (60 seconds). You can compare the number of records you have to the number of records you expect. \n",
    "\n",
    "First, connect to the pre-built CHAPTER09 database by issuing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "USE CHAPTER09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Then, determine the completeness of the Sensor data by counting the number of records and dividing that number by 60. If the returned value is greater than 1, this is an indicator your table has duplicate records. If less than 1, this indicates missing records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Chapter 9 / Deliverable 1\n",
    "############################\n",
    "\n",
    "SELECT COUNT(*)/62 AS ‘Completeness’ FROM Sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "In this case, the Completeness value is greater than 1—an indicator that your table has duplicate records. Later in this lab, you will learn how to quickly identify the duplicate records.\n",
    "\n",
    "Next, for each field within every record, you will want to determine the percentage of fields that do not have values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) FROM Sensor WHERE SensorValue IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find that NULL values exist, you can display those records by simply replacing ```COUNT(*)``` with ```*```, indicating that you would just like to SELECT all records, rather than SELECT them and perform a count on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Chapter 9 / Deliverable 2\n",
    "############################\n",
    "\n",
    "SELECT COUNT(*) FROM Sensor WHERE SensorValue IS NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for Duplicate Values\n",
    "\n",
    "Depending on the size of your dataset, quickly determining if duplicate records exists can seem like a challenging process—you likely cannot determine the duplicate records at a glance. To determine if the data set contains duplicate records, first determine the number of records the data set contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) AS 'Actual Record Count' FROM Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Next, determine the number of DISTINCT records in the set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(*) AS 'Distinct Record Count' FROM (Select DISTINCT * FROM Customers) c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "If the two queries return the same count, the data set does not have duplicate records. If the first query contains more records than the second, duplicate records exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to determine which records have more than the expected number, you would select all customers, count the number of records, group the customers by field, and then filter for records that appeared more than 1 time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT *, COUNT(*) FROM Customers\n",
    "GROUP BY CustomerID, FirstName, LastName\n",
    "HAVING COUNT(*) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "In this case, by grouping on three key fields, we have a very high probability that the records are the same. To be exact, you can group on all the fields. If you examine the Customers table, you would find the duplicate records. This query, however, would not detect a customer who is in the system twice, but with different customer IDs. To detect those customers, you would issue the following query that groups records on each field except the CustomerID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SELECT *, COUNT(*) FROM Customers\n",
    "GROUP BY FirstName, LastName, Age, Address, City, State, Zip, Birthday\n",
    "HAVING COUNT(*) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar operation would be performed to search for duplicate records in the Sensor table. And so we would begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Chapter 9 / Deliverable 3\n",
    "############################\n",
    "\n",
    "SELECT Record, SensorValue, COUNT(*) FROM Sensor\n",
    "GROUP BY Record, SensorValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Testing Value-Range Compliance\n",
    "\n",
    "Accuracy is a measure of the degree to which the data correctly represents the corresponding real-world values. Assume, for example, that a sensor should only produce values in the range 0 to 100. To begin your accuracy measurement, you should test to insure that all the data values are in the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "SELECT COUNT(SensorValue) AS 'Errant Value Count' FROM Sensor \n",
    "WHERE SensorValue < 0 OR SensorValue > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To display the actual values that fall outside of the expected value range, issue the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SELECT Record, SensorValue FROM Sensor \n",
    "WHERE SensorValue < 0 OR SensorValue > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Examining a Field’s Mean and Standard Deviation\n",
    "\n",
    "As you evaluate the quality of your data, you may want to perform similar processing to identify potentially errant data. Furthermore, you may want to perform simple statistics on a field’s value to determine if you should further examine the field values for errors and inconsistencies. For example, assume your data contains records for a monitor device. Under normal operations, the device should return values in the range 50 to 60. Using the SQL AVG (average) and STDDEV (standard deviation) functions, you can gain insight into the field’s values. \n",
    "\n",
    "For a device returning values in the range 50 to 60, you would expect a small standard deviation and a mean value close to 55. Should the standard deviation become large, you would likely want to perform further exploration of the field values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SELECT AVG(SensorValue), STDDEV(SensorValue) FROM Sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "In this case, the large standard deviation is an indication of outlier values. \n",
    "\n",
    "Using the AVG and STDDEV functions, you can identify specific records that have values that do not necessarily align with others. For example, the following query will identify values that differ from the average by more than the twice standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Chapter 9 / Deliverable 4\n",
    "############################\n",
    "\n",
    "SELECT Record, SensorValue FROM Sensor\n",
    "WHERE \n",
    "SensorValue > (SELECT AVG(SensorValue)+2*STDDEV(SensorValue) FROM Sensor) OR\n",
    "SensorValue < (SELECT AVG(SensorValue)-2*STDDEV(SensorValue) FROM Sensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The query tests the SensorValue against values that are 2 standard deviations above the average and 2 standard deviations less than the average. By examining values that are greater than twice the standard deviation from the mean, you are likely to identify only data outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Identifying Data Inconsistences\n",
    "\n",
    "Consistency is a measure of the degree to which similar or related data values align throughout the data set. For systems that store user-input values, data consistency is always a challenge. For example, assume that a call-center user must enter a user’s address:\n",
    "\n",
    "    123 North Main Street\n",
    "\n",
    "The user might enter any of the following to represent the address such as:\n",
    "\n",
    "    123 N Main Street\n",
    "\t123 N. Main Street\n",
    "\t123 No. Main Street\n",
    "\t123 North Main Street\n",
    "\t123 North Main St.\n",
    "\t123 North Main St\n",
    "\t123 N. Main St.\n",
    "\t123 N Main St.\n",
    "\t123 N Main St\n",
    "\t123 N. Main St\n",
    "\n",
    "Although a mailperson will deliver mail to any of the listed addresses and will consider each address as the same, an SQL query will not.\n",
    "\n",
    "Sticking with addresses, one user might specify a five-digit zip code (86305), whereas another user may input a nine-digit zip (86305-0001). Again, the data is conceptually the same, but not to an SQL query. \n",
    "Database developers refer to the process of correcting such data inconsistences as normalizing the data. Do not confuse this type of field normalization with the process of decomposing a relational table into first, second, or third normal form, as you may already be familiar with.\n",
    "As you might guess, because they are often entered by users, addresses are constantly a data-consistency challenge. To start, you might test to confirm that each address uses the same zip code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "SELECT A.Address, A.Zip, B.Zip FROM Customers A\n",
    "INNER JOIN (SELECT Address, Zip FROM Customers) B\n",
    "ON A.Address = B.Address WHERE A.Zip != B.Zip \n",
    "GROUP BY A.Address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The query performs a self-join (an inner join on itself, based on the address field). Then, it compares the zip codes of the joined records, returning those that differ. As you can see, the query uses table aliases to specify fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Eliminating Leading and Trailing Spaces\n",
    "When a user enters data into a form, which a database later stores, there may be times when the user errantly types leading or trailing spaces before or after the values they type. Ideally, the application program storing the data will recognize and eliminate these extra characters. That said, as part of your data-cleansing process, you may need to eliminate them. In such cases, you can leverage the SQL RTRIM and LTRIM functions to remove the spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "UPDATE Customers SET Firstname=RTrim(LTrim(FirstName)),\n",
    "Lastname=RTrim(LTrim(LastName)),\n",
    "Address=RTrim(LTrim(Address)),\n",
    "City=RTrim(LTrim(City)),\n",
    "State=RTrim(LTrim(State)),\n",
    "Zip=RTrim(LTrim(Zip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "In this case, the UPDATE query will assign the specified fields with the value each contains minus leading spaces, which are removed by LTRIM and without the trailing spaces which are removed by RTRIM. When you execute the query, SQL will display the number of records it updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Creating Your Own Custom Data Wrangler in Python\n",
    "\n",
    "Depending on the complexity of the data they must transform or cleanse, database developers can use a programming language, such as Python, to create their own custom data-wrangling application. In this section, you will use Python to perform simple data-wrangling operations. You won’t need to create a Python program, in this case, but rather, you can execute the program statements using the Python interpreter. The purpose of this section is not to make you a Python programmer, but rather, to give you a feel for Python’s data cleansing capabilities. \n",
    "\n",
    "In this context, the Python package pandas has been pre-installed for you. However, should you have been required to install this package on your own, you would issue the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "```python\n",
    "! pip install --user pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To start, you would typically import the pandas library, as powerful Python software library used for data manipulation and analysis. You would do this using the following line:\n",
    "```python\n",
    "import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Next, you would direct Python to create a dataframe to hold your data and to load into it the contents of the NeedsWrangling.csv file:\n",
    "```python\n",
    "dataFile = pd.read_csv('NeedsWrangling.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Once you have loaded your dataset into a dataframe object, you could begin performing actions on it. For example, using Python’s print method, you can display the number of rows and columns your data contains:\n",
    "```python\n",
    "print(dataFile.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Chapter 9 / Deliverable 6\n",
    "############################\n",
    "\n",
    "# import pandas here\n",
    "# load NeedsWrangling.csv into a dataframe object\n",
    "# Print the number of rows and columns in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To display the first five rows in your data, use the Python head method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the head method displays the first 5 rows by default. To specify the number of rows to display, you would pass an integer value to the method.\n",
    "\n",
    "The NeedsWrangling.csv file contains duplicate records. To view the duplicated records, use the duplicated method. The method returns a list of Boolean values that specify whether a row is a duplicate. The example below returns the first 7 rows with their corresponding Boolean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Chapter 9 / Deliverable 7\n",
    "############################\n",
    "\n",
    "dupList = dataFile.duplicated()\n",
    "dupList.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To remove the duplicate records, use the drop_duplicates method and assign the result to the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = dataFile.drop_duplicates('CustomerID')\n",
    "dataFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Several of the fields within the NeedsWrangling.csv file have leading and trailing spaces. To eliminate such whitespace from a column, perform the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile['Firstname'] = dataFile['Firstname'].map(str.strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the State column, you will find that some records use an uppercase value, some lowercase, and possibly some mixed case. Maybe you want to capitalize all the values in the a column, in which case you could use the upper() method. Or perhaps want to capitalize the first letter of each word in a column, in which case you could use the capitalize() method. The following example useds upper() to capitalize all letters in the State column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Chapter 9 / Deliverable 8\n",
    "############################\n",
    "\n",
    "dataFile['State'] = dataFile['State'].map(lambda s : s.upper())\n",
    "dataFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As discussed earlier, user-entered addresses and zip codes often yield inconsistent data. One way to identify potential problems is to display a count of values. The following operation displays a count of the different zip codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile['Zip'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Often, as you analyze data, you will want to sort the data based on a column’s values. The following command directs Pandas to sort the data values by Lastname:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile.sort_values('Lastname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the following statement directs Pandas to display only the Address column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile[\"Address\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Finally, analysts often want to know if NULL or NaN (Not a Value) values exist. Using the isnull method, you can direct pandas to report whether or not a field is NULL or NaN. By default, Pandas will return a true or false value for every field. As an alternative, you can use the sum method to add up any such values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
