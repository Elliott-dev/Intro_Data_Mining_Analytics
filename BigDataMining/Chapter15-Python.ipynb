{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Chapter: 15 Big Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The term “big data” applies to datasets that exceed the size our data programs can hold. For Excel, that might be a dataset that is larger than the available memory, and for MySQL, that might be the size of the largest file supported by the underlying operating system. The point is, most software applications have size limits which the size of big-data solutions exceed.\n",
    "\n",
    "Normal questions you may have probably include: where is all the big data coming from, how quickly, and how much? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Bring the Process to the Data\n",
    "\n",
    "When programs retrieve data from a disk, the time required to get the data from the disk directly impacts the application’s performance. When you distribute files across a network, the network’s transmission speed will become the longest delay in the process.\n",
    "\n",
    "Assume, for example, that you must read a 10GB file to find each occurrence of the word “Data Mining.” On a local drive, a system may read the file (based on a 100MB/S disk) in about 100 seconds. If you move the file to a distributed file system, the network-transportation speed will increase that amount of time. \n",
    "\n",
    "Developers refer to solutions that retrieve all the data for a file for processing in this way as “bringing the data to the process.” The time to perform such processing is dependent on the amount of data retrieved.\n",
    "\n",
    "Hadoop changes this model to instead “bring the process to the data.” In the Hadoop model, rather than returning all the data and then performing the processing, Hadoop sends the processing to be performed to each node and lets each perform the operation to collect and return the desired data. In this way, if you break apart the previous 10GB file across 20 nodes, Hadoop can parallelize the operations, significantly reducing the processing time required--in this case, down to 5 to 10 seconds. If you further distribute the file across more nodes, such as 100, you can quickly distribute away much of the processing time. As an example, estimates place the number of Hadoop nodes driving the Yahoo search-index page at over 10,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Enter Map Reduce\n",
    "\n",
    "As you have learned, HDFS, the Hadoop Distributed File System, distributes large files across many nodes within a cluster. To perform big-data analytics, applications normally use a two-step process called MapReduce. \n",
    "\n",
    "During the first step, the application maps the data that it reads into key-value pairs. Assume, for example, you have stock-market data for technology stocks (date, symbol, open, low, high, and close) since the year 2000. Your goal is to know the date of each company’s highest closing stock price.\n",
    "\n",
    "Our data of interest, in this case, are the date of each record and the closing price. The Map phase will identify the key-value pair of interest (date, high price). The query, as such, would provide data for each record. The Reduce phase, then, will examine that data to determine the desired result (the date and price of the highest price found).\n",
    "\n",
    "To implement MapReduce processing, developers define two functions, one to perform the mapping and one to perform the reduction. Using Hadoop, developers can implement these functions using Java, C++, Python, and other languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Looking at a Simple MapReduce Example\n",
    "\n",
    "To better understand the MapReduce process, consider the following simple Python scripts that use the data file stocks.csv. \n",
    "The first script, Mapper.py, reads the file, searching for the MSFT symbol and outputs matching dates and closing prices. That script consists of the following code:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    data = line.split(',')\n",
    "    if (data[0] == 'MSFT'):\n",
    "      print(data[1] + ',' + data[2])\n",
    "```\n",
    "\n",
    "To run the script, you can redirect the contents of the Stock.csv file to the script, as is performed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python Mapper.py < Stocks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Mapper returns the data for each MSFT record along with the closing price, but does not calculate the date of the highest price—that’s the role of the Reduce function, which the script, Reduce.py, implements:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "\n",
    "high = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    data = line.split(',')\n",
    "    close = float(data[1].rstrip())   # get rid of newline\n",
    "\n",
    "    if (close > high):\n",
    "      high = close\n",
    "      date = data[0]\n",
    "\n",
    "print(date + ',' + str(high))\n",
    "```\n",
    "To combine the scripts, in order to perform both the mapping and reducing operations on the dataset, we would use the following line:\n",
    "\n",
    "```python\n",
    "! python Mapper.py < Stocks.csv | python Reduce.py\n",
    "```\n",
    "As you can see, first, we perform the mapping operation on the Stocks.csv file, returning Microsoft's highest closing price for each date; then, we send the results to our reduce script, which finds Microsoft's highest closing price, and returns its data and price values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Chapter 15 / Deliverable 1\n",
    "#############################\n",
    "\n",
    "! python Mapper.py < Stocks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As you may discover, this command displays the date and closing price for Microsoft’s highest closing price.\n",
    "Admittedly, this example was very simple. In a real-world example, Hadoop would pass the functions to each node, which, in turn, would use it to process its data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing MapReduce Options Using MongoDB\n",
    "\n",
    "This section performs operations on a database that has been pre-built for you. The commands used to build this database can be found in the chapter15-database.js script file located in the chapter folder.\n",
    "\n",
    "*Note: None of the code found in this section is valid in the Jupyter environment. You are to run the code in the MongoDB Server instance you prepared in *Getting Started with MongoDB.*\n",
    "\n",
    "Like Hadoop, MongoDB also offers distributed data solutions through a mechanism called sharding. MongoDB can distribute data across nodes (\"shards\") and then access the data through a router that maps data requests to the correct shard. To provide support for big-data analytics, MongoDB provides the mapReduce command, which enables MapReduce aggregation operations over sharded collections. \n",
    "\n",
    "First, connect to the pre-built Stocks database by issuing the following command in the MongoDB Shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Trusted": true,
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "use Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Then, to perform a MapReduce operation you must define two functions: map and reduce. Execute the following query in the MongoDB Shell to create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "var map = function() {\n",
    "  emit(this.Symbol, { date: this.Date, close: this.Close});\n",
    "};\n",
    "\n",
    "reduce = function (key, values) {\n",
    "  output = { date: \"\", close: 0 }\n",
    "\n",
    " for (i = 0; i < values.length; i++)\n",
    "    if (values[i].close > output.close)\n",
    "      {\n",
    "        output.close = values[i].close;\n",
    "        output.date = values[i].date;\n",
    "      }\n",
    "\n",
    "  return(output);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The map function provides the stock symbol, date, and closing price for each record. The reduce function receives a group of related records (same key) and determines and returns the highest stock closing price within the group. Note that the reduce function returns a result in the form emitted by the map function. That’s because the reduce function may be called with partial group data (such as some of the MSFT data) and will be called repeatedly until the aggregation is complete. \n",
    "\n",
    "To use these two functions to perform a Map Reduce operation, use the mapReduce class. Execute the following query into MongoDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 15 / Deliverable 2\n",
    "######################################\n",
    "\n",
    "db.Data.mapReduce(map, reduce, { out: { inline: 1}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
