{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Chapter 11: Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "When data analysts mine data, they must often assign a classification label to data (such as a pet is a dog, cat, horse) or determine if an object is or isn’t something (such as a loan being approved or disapproved). With respect to data mining, classification is the use of a supervised machine-learning algorithm to assign an observation into a specific category. Depending on the application, the classification may be binary (two classes):\n",
    "\n",
    "    •\tIs the customer a male or female?\n",
    "    •\tIs the pet a dog or a cat?\n",
    "    •\tShould the loan be approved or disapproved?\n",
    "    •\tIs the wine a red or the white?\n",
    "    •\tIs the tumor benign or malignant?\n",
    "\n",
    "Or, the classification may use multiple classes (categories):\n",
    "\n",
    "    •\tIs a person’s facial expression happy, sad, angry?\n",
    "    •\tWhat breed of dog is my pet?\n",
    "    •\tWhich user’s biometric data was entered?\n",
    "    •\tWhich voice-recognition option did the caller select?\n",
    "    •\tHow do we predict the book to sell (poorly, good, very good, great)?\n",
    "\n",
    "Classification algorithms work by examining an input “training set” of data to learn how the data values combine to create a result. Such a training set, for example, might contain heights, weights, colors, and temperaments of different dogs and the resulting breeds, or it might contain the sizes, shapes, dimensions, and locations of tumors that are malignant as well as similar data for tumors that are benign. In other words, the training data contains predictive values and the correct classification results.  \n",
    "\n",
    "After the learning algorithm learns from and models the training test data, a “test” data set (for which the correct results are known) is tested against the model to determine its accuracy, such as 97%. With knowledge of the accuracy in hand, the data analyst can then use the model to classify other data values.\n",
    "Normally, the training set and testing set come from the same dataset of values that are known to be correct or observed. The data analyst will specify, for example, that 70% of the data will be training data and 30% will be used for testing. Across the Web, you can find many different datasets of known or observed data that you can use to try different classification algorithms. This notebook will use several commonly-used algorithms to:\n",
    "\n",
    "    •\tDetermine to which type of Iris a flower belongs.\n",
    "    •\tDetermine if a breast-cancer tumor is malignant or benign.\n",
    "    •\tDetermine, based on chemical composition, a wine’s type.\n",
    "    •\tDetermine whether breast-surgery patients should live five or more years post surgery.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the scripts presented in this notebook use several Python libraries which have been pre-installed for you. If you had been required to install these libaries on your own, you would issue the following commands:\n",
    "\n",
    "```python\n",
    "! pip install --user pandas\n",
    "! pip install --user numpy\n",
    "! pip install --user sklearn\n",
    "! pip install --user matplotlib\n",
    "! pip install --user graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Applying the K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "The KNN’s classification algorithm is based on the premise “If it walks like duck and quacks like a duck, it’s a duck.” To use KNN, you provide a value for the number K that specifies the number of neighboring data-set values to which a value must be similar in order to be considered part of a group.\n",
    "\n",
    "When you use the KNN algorithm to classify data, you must specify the value of K for the number of neighbors to which a point must be similar in order to be included in a group. If you specify a value of K that is too small, you may “overfit” the model, meaning the model may start to treat noise or errant data as valid training data. Likewise, if you specify too large a value for K, you may “underfit” the model, which means the model is not capable of correctly modeling the training data.\n",
    "\n",
    "The following Python script, IrisKNN.py, opens the Iris dataset and loads the data into two arrays, one containing the petal and sepal data (X) and one containing the known classifications (y). The code then splits the arrays into a training dataset that contains 70% of the values and a testing dataset that contains the remaining 30%. \n",
    "The script then uses the KNN (K-nearest neighbors) algorithm with K=3, to calculate and display the model’s accuracy. Using the model, the script then predicts the classification for three sets of sepal and petal lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 1\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(\"\\nFrom the test data\")\n",
    "print('Index\\tPredicted\\t\\tActual')\n",
    "for i in range(len(pred)):\n",
    "  if pred[i] != y_test[i]:\n",
    "    print(i, '     ', pred[i],'       ', y_test[i], ' ****')\n",
    "    \n",
    "DataToPredict = np.array([[5.2,3.5,1.4,0.2],[5.7,2.9,3.6,1.3],[5.8,3.0,5.1,1.8]])\n",
    "pred = knn.predict(DataToPredict)\n",
    "\n",
    "print(\"\\nPredicted Results\")\n",
    "for i in range(len(pred)):\n",
    "    print(DataToPredict[i], pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To help you understand which of the model's predictions were correct and which predictions were wrong, you can display the model’s confusion matrix, which summarizes the prediction’s results.\n",
    "\n",
    "The following Python script, ConfusionIrisKNN.py, uses KNN to model the Iris-flower data. The script displays the accuracy score and the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(\"\\nFrom the test data\")\n",
    "print('Index\\tPredicted\\t\\tActual')\n",
    "for i in range(len(pred)):\n",
    "  if pred[i] != y_test[i]:\n",
    "    print(i, '\\t', pred[i], '\\t', y_test[i], ' ****')\n",
    "    \n",
    "print('\\nConfusion Matrix\\n', confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Predicting Wine Types\n",
    "\n",
    "The Wine dataset, available at University of California (UCI) Data Repository, contains 13 attributes that contribute to the quality of wine. The dataset contains data for three types of wines, identified by the category values 1, 2, and 3. The dataset consists of 178 records. \n",
    "\n",
    "The following Python script, WineKNN.py, uses the Wine data set with K=5 to predict the types for 3 sets of wine values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 2\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['class', 'Alcohol','Malic Acid','Ash','Acadlinity','Magnisium','Total Phenols','Flavanoids', 'NonFlavanoid Phenols', 'Proanthocyanins', 'Color Intensity', 'Hue', 'OD280/OD315', 'Proline' ]\n",
    "\n",
    "df = pd.read_csv('wine.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 1:14])\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "    \n",
    "DataToPredict = np.array(\n",
    "[[14.23,1.71,2.43,15.6,127,2.8,3.06,.28,2.29,5.64,1.04,3.92,1065],\n",
    "[12.64,1.36,2.02,16.8,100,2.02,1.41,.53,.62,5.75,.98,1.59,450],\n",
    "[12.53,5.51,2.64,25,96,1.79,.6,.63,1.1,5,.82,1.69,515],\n",
    "[13.49,3.59,2.19,19.5,88,1.62,.48,.58,.88,5.7,.81,1.82,580]])\n",
    "\n",
    "pred = knn.predict(DataToPredict)\n",
    "\n",
    "print(\"\\nPredicted Results\")\n",
    "for i in range(len(pred)):\n",
    "    print('\\t', DataToPredict[i], '\\t', 'Class: ', pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Predicting Breast Cancer Malignancy Using KNN\n",
    "\n",
    "The breast-cancer dataset, available at the University of California Irvine (UCI) Data Repository contains 32 attributes which can be used to determine if a breast-cancer tumor is malignant or benign. The dataset contains 569 records.\n",
    "\n",
    "The following Python script, CancerPredictKNN.py, uses the dataset with K=5 to train and test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 3\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 1:9])  # do not include sample field\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Classification Using Naïve Bayes\n",
    "\n",
    "There are many different classification algorithms, each of which apporaches the data-grouping process differently. The Naïve Bayes classification algorithm is so named because it is based on Bayes Theorem to calculate the probability that an item is a member of a category based upon knowledge of related conditions. The Naïve Bayes classification algorithm is called “naïve” in that it treats the different dataset attributes as independent and calculates a probability for each.\n",
    "\n",
    "The following Python script, NaiveBayes.py, uses the GaussianNB function to predict which class of Iris a flower observation aligns with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "model = GaussianNB().fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(\"\\nFrom the test data\")\n",
    "print('Index\\tPredicted\\t\\tActual')\n",
    "for i in range(len(pred)):\n",
    "  if pred[i] != y_test[i]:\n",
    "    print(i, '\\t', pred[i], '\\t', y_test[i], ' ****')\n",
    "    \n",
    "DataToPredict = np.array([[5.2,3.5,1.4,0.2],[5.7,2.9,3.6,1.3],[5.8,3.0,5.1,1.8]])\n",
    "pred = model.predict(DataToPredict)\n",
    "\n",
    "print(\"\\nPredicted Results\")\n",
    "for i in range(len(pred)):\n",
    "    print('\\t', DataToPredict[i], '\\t', pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As discussed, Naïve Bayes will create probabilities for each attribute. To display the probabilities, you can use the predict_proba function, as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "\n",
    "model = GaussianNB().fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print('probabilities')\n",
    "print (model.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Classification Using Logistic Regression\n",
    "\n",
    "The logistic-regression classifier is best suited for binary-dependent variables—meaning classifications for which there are only two classes, such as gender, a tumor being malignant or benign, and so on. That said, you can use Logistic Regression for multiclass problems, however, your results may not prove as accurate as other methods. \n",
    "\n",
    "A logistic-regression classifier does not use the dependent variable (the classes we are trying to group into) directly; rather, it employs a function that uses each of the predictor variables called a logit. The logistic-regression algorithm is often called the “logit” algorithm. Behind the scenes, the algorithm uses a series of odds that correspond to whether an event will occur. The logistic classifier determines the probability that data belongs to each class based upon this series of odds which it produces by analyzing each predictor variable. \n",
    "\n",
    "The following Python script, LogitisticRegressionIris.py, uses the model to predict Iris flower types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 4\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=200).fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy score: ', accuracy_score(y_test, pred))\n",
    "\n",
    "print('\\nIndex     Predicted         Actual')\n",
    "for i in range(len(pred)):\n",
    "  if pred[i] != y_test[i]:\n",
    "    print(i, '      ',pred[i], '  ', y_test[i], ' ****')\n",
    "    \n",
    "DataToPredict = np.array([[5.2,3.5,1.4,0.2],[5.7,2.9,3.6,1.3],[5.8,3.0,5.1,1.8]])\n",
    "pred = model.predict(DataToPredict)\n",
    "\n",
    "print(\"\\nPredicted Results\")\n",
    "for i in range(len(pred)):\n",
    "    print(DataToPredict[i], pred[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As stated, Logistic Regression is best suited for a binary-dependent variable. The following Python script, LogisticRegressionCancer.py, uses the approach to predict if a tumor is benign or malignant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 5\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:9]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs').fit(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print('Accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Classification Using a Neural Network\n",
    "\n",
    "Neural networks are at the heart of machine learning and are used for a wide range of applications, including classification. In this section, you will examine the MLPClassifer function, so named because it uses multilayer perceptrons to accomplish its processing, in this case classifications.\n",
    "\n",
    "In a neural network, a perceptron is a supervised learning algorithm that uses a linear function to convert inputs into outputs. However, many real-world problems are not linear in nature. As such, the problems must be decomposed into a series of linear components, and additional layers of perceptrons must be used.\n",
    "\n",
    "The following Python script, MLPIris.py, uses a multilayer-perceptron model to predict Iris flower types. MLP will iterate through the dataset until either convergence occurs (when the score is no longer improving over a certain period), or the specified maximum number of iterations is reached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 6\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print ('Accuracy: ', accuracy_score(y_test, pred))\n",
    "\n",
    "print('Index\\t  Predicted\\t   Actual')\n",
    "for i in range(len(pred)):\n",
    "  if pred[i] != y_test[i]:\n",
    "    print(i, '       ', pred[i], ' ', y_test[i], ' ****')\n",
    "    \n",
    "\n",
    "DataToPredict = np.array([[5.2,3.5,1.4,0.2],[5.7,2.9,3.6,1.3],[5.8,3.0,5.1,1.8]])\n",
    "pred = model.predict(DataToPredict)\n",
    "\n",
    "print(\"\\nPredicted Results\")\n",
    "for i in range(len(pred)):\n",
    "    print('\\t', DataToPredict[i], '\\t', pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Classification Using Decision Trees\n",
    "\n",
    "A decision tree is a graph-based data structure that a program can use to follow a series of decision paths to arrive at a decision. \n",
    "\n",
    "Within machine learning, a decision-tree classifier creates a similar structure with decision points that are based upon the different dataset attributes. As you might guess, as the number and complexity of the attributes increase, so too does the complexity of the underlying decision tree.\n",
    "\n",
    "The following Python script, DecisionTreeCancer.py, uses a decision tree to predict if a breast-cancer tumor is benign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 7\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.tree as tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:10]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "pred = DT.predict(X_test)\n",
    "print ('Accuracy Score: ', accuracy_score(y_test, pred))\n",
    "print('Confusion Matrix\\n', confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Classifying Data Using Random Forests\n",
    "\n",
    "In the previous section you learned how to use decision-tree modeling to classify data. Depending on the dataset and model, there may be times when the decision tree becomes very deep (many levels of nodes). Often, such decision trees will overfill the data and have will have a large variance. \n",
    "A random-forest classification model is similar, but at each split, only considers a random subset of the attributes.\n",
    "\n",
    "The following Python script, RandomForests.py, uses a random forest to predict whether a tumor is benign or malignant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 8\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.tree as tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:10]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print ('Accuracy Score: ', accuracy_score(y_test, pred))\n",
    "print('Confusion Matrix\\n', confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Classifying Data Using a Support Vector Machine\n",
    "\n",
    "The Support Vector Machine (SVM), often called SVC (Support Vector Classifier) classifies data by separating values with a line called a hyperplane. The SVC algorithm extends the separation capabilities to support such non-linear solutions. \n",
    "SVC is ideal for binary-classification problems, such as whether a loan will be approved or disapproved. That said, you can use SVC to multiclass problems; however your solution may not be as accurate as other methods.\n",
    "\n",
    "The following Python script, for example, SVCIris.py, uses SVC to predict Iris-flower types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) \t\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "model = SVC(gamma='auto').fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(\"\\nFrom the test data\")\n",
    "print('Index\\tPredicted\\t\\tActual')\n",
    "for i in range(len(pred)):\n",
    "  if pred[i] != y_test[i]:\n",
    "    print(i,'     ', pred[i], '       ', y_test[i], ' ****')\n",
    "    \n",
    "DataToPredict = np.array([[5.2,3.5,1.4,0.2],[5.7,2.9,3.6,1.3],[5.8,3.0,5.1,1.8]])\n",
    "pred = model.predict(DataToPredict)\n",
    "\n",
    "print(\"\\nPredicted Results\")\n",
    "for i in range(len(pred)):\n",
    "    print(DataToPredict[i], pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As discussed, SVC is ideal for binary classifications. The following Python script, SVCcancer.py, uses SVC to predict whether a breast tumor is malignant or benign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 11 (Python) / Deliverable 9\n",
    "######################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "names = ['Sample', 'Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "\n",
    "df = pd.read_csv('breast.data.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 1:9]) # designate the attributes to be used in the classification\n",
    "y = np.array(df['class'])\n",
    "\n",
    "# split the data into the train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "model = SVC(gamma='auto').fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print ('Model accuracy score: ', accuracy_score(y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
