{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Chapter: 10 Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Some of the scripts presented in this notebook use several Python libraries which have been pre-installed for you. If you had been required to install these libaries on your own, you would issue the following commands:\n",
    "\n",
    "```python\n",
    "! pip install --user pandas\n",
    "! pip install --user numpy\n",
    "! pip install --user sklearn\n",
    "! pip install --user statistics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Determining the Number of Clusters\n",
    "\n",
    "When you use the K-means algorithm, you must specify the value of K, meaning the number of clusters you desire. If you specify too few clusters, you may lose valuable insights. Likewise, if you specify too many clusters, you will increase your processing time and you may not gain additional insights. \n",
    "\n",
    "You will need to determine and specify the number of clusters for each dataset with which you work. Depending on the dataset values, you may find that for one set of values (possibly from the same data source), a cluster size of 3 is appropriate, whereas for other values, a cluster size of 5 provides better grouping. The only way to determine the appropriate cluster size is to create clusters and then to analyze the results (normally using the sum of squared distances). \n",
    "\n",
    "Several algorithms exist to help you determine the proper number of clusters for your data. A common approach is called the “elbow method,” so named because the chart that it produces resembles the bend in an elbow. In this case, the bend at the elbow (the point where adding more clusters has minimal impact) occurs at 3 clusters. You create the elbow chart by charting the sum of the squares of each cluster result.\n",
    "\n",
    "The following Python program, Elbow.py, creates an elbow chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 1\n",
    "######################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas import DataFrame\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "\n",
    "distances = []\n",
    "\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    ClusterInfo = kmeanModel = KMeans(n_clusters=k).fit(df)\n",
    "    distances.append(ClusterInfo.inertia_)\n",
    "\n",
    "plt.plot(K, distances, 'bo-')\n",
    "plt.xlabel('K-Clusters')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Cluster Values and Distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using K-Means Clustering\n",
    "\n",
    "The “means” in K-means clustering corresponds to the average distance for each point in the cluster to the cluster’s center (centroid). K-means is an iterative algorithm that loops until either the maximum number of iterations is reached, or, the clusters do not change. To start the the k-means clustering process, you will specify the number of clusters, the maximum number of iterations, and the starting location for k centroids (cluster centers for which you will normally specify k-random values). The locations that you choose for the starting centroids, as specified, can be random. The K-means algorithm will move the centroids to the ideal locations as it performs it processing. With each iteration, the K-means algorithm will perform these steps:\n",
    "\n",
    "    •\tCalculate k-centroid locations\n",
    "    •\tMove each point into the nearest cluster\n",
    "\n",
    "In other words, with each iteration, the algorithm will move each cluster’s centroid to the location that minimizes the average distance to the cluster’s points. The following Python script, KMeans.py, creates a 3-cluster grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 2\n",
    "######################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas import DataFrame\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "  \n",
    "kmeans = KMeans(n_clusters=3).fit(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "plt.scatter(df['x'], df['y'], c=kmeans.labels_.astype(float))\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Using K-Means++\n",
    "\n",
    "When you use the K-means algorithm, you normally specify the starting centroid locations as random values. The K-means++ algorithm improves processing time by better calculating the starting centroid locations. The following Python program, KMeansPlusPlus.py, using K-means to create the same 3-cluster grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 3\n",
    "######################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas import DataFrame\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "  \n",
    "kmeans = KMeans(n_clusters=3, init='k-means++').fit(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "plt.scatter(df['x'], df['y'], c=kmeans.labels_.astype(float))\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "As you can see, the program passes the parameter init=’k-means++’ to the KMeans function.\n",
    "K-means++ should arrive at a solution faster than K-means. The following Python script, TimeClusters.py, uses the K-means and K-means++ algorithms to create clusters with K=3, K=4, and K=5, timing the processing required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 4\n",
    "######################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas import DataFrame\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "\n",
    "KMeansStartTime = time.time()\n",
    "   \n",
    "kmeans = KMeans(n_clusters=3).fit(df)\n",
    "kmeansDistance = kmeans.inertia_\n",
    "\n",
    "kmeans = KMeans(n_clusters=4).fit(df)\n",
    "kmeansDistance += kmeans.inertia_\n",
    "\n",
    "kmeans = KMeans(n_clusters=5).fit(df)\n",
    "kmeansDistance += kmeans.inertia_\n",
    "\n",
    "KMeansStopTime = time.time()\n",
    "\n",
    "KMeansppStartTime = time.time()\n",
    "   \n",
    "kmeans = KMeans(n_clusters=3, init='k-means++').fit(df)\n",
    "kmeansppDistance = kmeans.inertia_\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, init='k-means++').fit(df)\n",
    "kmeansppDistance += kmeans.inertia_\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++').fit(df)\n",
    "kmeansppDistance += kmeans.inertia_\n",
    "\n",
    "KMeansppStopTime = time.time()\n",
    "\n",
    "print('KMeans time ', KMeansStopTime - KMeansStartTime)\n",
    "print('KMeans total distance ', kmeansDistance)\n",
    "print('KMeans++ time ', KMeansppStopTime - KMeansppStartTime)\n",
    "print('KMeans++ total distance ', kmeansppDistance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "A hierarchical-clustering algorithm takes a different approach to grouping data. There are two forms of hierarchical-clustering algorithms: bottom-up and top-down. The bottom-up-clustering algorithm is called an agglomerative algorithm because, with each iteration, it merges related clusters into a larger cluster. In other words, the bottom-up algorithm finds the two nearest clusters and merges them, repeating this process until only one cluster exists.\n",
    "\n",
    "In contrast, a top-down hierarchical-clustering algorithm starts with one cluster and with each iteration, decomposes the cluster to form the lower-level clusters. Because it breaks apart a larger cluster into smaller clusters, the top-down approach is called a divisive algorithm. To understand how the hierarchical algorithm groups clusters, analysts use a chart,  called a dendrogram, to show the cluster groupings. \n",
    "\n",
    "The previous discussion used the minimum distance between points to select the points assigned to a cluster. It turns out that hierarchical algorithms can use several different approaches to selecting points:\n",
    "\n",
    "    •\tSimple linkage: Select the closest neighbor.\n",
    "    •\tComplete linkage: Selects points furthest apart.\n",
    "    •\tWards: Selects the point that results in the smallest increase to the group’s sum of squares.\n",
    "    •\tAverage linkage: Selects points to minimize the average distance between points.\n",
    "\n",
    "The following Python script, MultiDendrogram.py, creates the dendrograms using each method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "\n",
    "dendrogram(linkage(df, 'ward'))\n",
    "plt.title('Ward')\n",
    "plt.show()\n",
    "\n",
    "dendrogram(linkage(df, 'single'))\n",
    "plt.title('Single')\n",
    "plt.show()\n",
    "\n",
    "dendrogram(linkage(df, 'complete'))\n",
    "plt.title('Complete')\n",
    "plt.show()\n",
    "\n",
    "dendrogram(linkage(df, 'average'))\n",
    "plt.title('Average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The following Python program, HierchicalCharts.py, produces the cluster charts for each point-selection algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \n",
    "cluster.fit_predict(df)  \n",
    "plt.scatter(df['x'], df['y'], c=cluster.labels_, cmap='rainbow')  \n",
    "plt.title('Ward');\n",
    "plt.show()\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='single')  \n",
    "cluster.fit_predict(df)  \n",
    "plt.scatter(df['x'], df['y'], c=cluster.labels_, cmap='rainbow')  \n",
    "plt.title('Single')\n",
    "plt.scatter(df['x'], df['y'], c=cluster.labels_, cmap='rainbow')  \n",
    "plt.show()\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete')  \n",
    "cluster.fit_predict(df)  \n",
    "plt.scatter(df['x'], df['y'], c=cluster.labels_, cmap='rainbow')  \n",
    "plt.title('Complete')\n",
    "plt.show()\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='average')  \n",
    "cluster.fit_predict(df)  \n",
    "plt.scatter(df['x'], df['y'], c=cluster.labels_, cmap='rainbow')  \n",
    "plt.title('Average')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# DBSCAN Clustering\n",
    "\n",
    "DBSCAN, which stands for Density-Based Spatial Clustering of Applications with Noise, groups points within clusters based on the density of surrounding points. Within a cluster, a “core point” has at least the minimum number of points surrounding it (min_samples) within the given radius (eps). A “cluster border point,” in contrast, falls within the radius distance, but does not have the minimum number of points surrounding it. All other points fall outside of the cluster and are considered “noise” (represented by the purple dots in the script below).\n",
    "\n",
    "The DBSCAN algorithm starts by determining the point types (core, border, and noise). It then creates a cluster for each core point, merging the clusters that fall within the radius. Finally, DBSCAN adds the border points to the cluster.\n",
    "The following Python script, DBSCAN.py, uses the DBSCAN algorithm to group data into clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 5\n",
    "######################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pandas import DataFrame\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "\n",
    "clustering = DBSCAN(eps=3, min_samples=3).fit(df)\n",
    "labels = clustering.labels_\n",
    "numberofclusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "plt.title('DBSCAN Number of clusters: %d' % numberofclusters)\n",
    "plt.scatter(df['x'], df['y'], c=clustering.labels_.astype(float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Interesting Cluster Shapes\n",
    "\n",
    "When data analysts first start clustering data, they often envision clusters as neat and orderly groups. Clusters, however, can take on a variety of shapes and form. The following Python program, Moons.py, creates the clusters using the make_moons and make_circles datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 6\n",
    "######################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_moons(n_samples=500)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5).fit(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_.astype(float))\n",
    "plt.show()\n",
    "\n",
    "X, y = datasets.make_moons(n_samples=500, noise=0.05)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5).fit(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_.astype(float))\n",
    "plt.show()\n",
    "\n",
    "X, y = datasets.make_circles(n_samples=500)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5).fit(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_.astype(float))\n",
    "plt.show()\n",
    "\n",
    "X, y = datasets.make_circles(n_samples=500, noise=0.05)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5).fit(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_.astype(float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Viewing Cluster Assignments\n",
    "\n",
    "When you create your clusters, the functions will normally return a vector of values that specify to which cluster the corresponding point has been assigned. When you plot the clusters, your plotting functions will use this vector to assign different colors to each cluster. The following Python script, ShowClusters.py, prints the cluster vector returned by the Kmeans function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 7\n",
    "######################################\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from pandas import DataFrame\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "  \n",
    "kmeans = KMeans(n_clusters=3).fit(df)\n",
    "\n",
    "clusters = kmeans.labels_\n",
    "i = 0\n",
    "print(\"Cluster\\t X\\t Y\")\n",
    "for row in df.iterrows():\n",
    "   print(clusters[i],'      ', row[1]['x'],'    ', row[1]['y'])\n",
    "   i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Similarly, this Python script, ShowHierarchicalClusters.py, displays the clusters returned for a hierarchical clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 8\n",
    "######################################\n",
    "\n",
    "from pandas import DataFrame\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \n",
    "cluster.fit_predict(df)  \n",
    "\n",
    "clusters = cluster.labels_\n",
    "i = 0\n",
    "print(\"Cluster\\t X\\t Y\")\n",
    "for row in df.iterrows():\n",
    "   print(clusters[i], '      ', row[1]['x'],'    ', row[1]['y'])\n",
    "   i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Identifying Data Outliers\n",
    "\n",
    "An outlier is a value that falls outside of the expected range of values. Depending on the analysis you are performing, the presence of one or more outliers can have a significant impact on your results. The following Python program, BasicMetrics.py, calculates the mean and standard deviation for an array of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    " \n",
    "values = [-100, -75, 1,2,3,4,5, 75, 100]\n",
    "print('Mean', statistics.mean(values))\n",
    "print('Standard Deviation', statistics.stdev(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "In this case, the large standard deviation, relative to the mean, is an indication that outlier values may exist. Next, the following program, IdentifyOutliers.py, examines the array values to identify values that fall outside of the standard deviation from the mean, and if so, identifies the corresponding value and index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    " \n",
    "values = [-100, -75, 1,2,3,4,5, 75, 100]\n",
    "mean = statistics.mean(values)\n",
    "stdev = statistics.stdev(values)\n",
    "\n",
    "print('Mean ', mean)\n",
    "print('Standard deviation ', stdev) \n",
    "\n",
    "newvalues = []\n",
    "\n",
    "for i in range(len(values)):\n",
    "  if values[i] < (mean - stdev) or values[i] > (mean + stdev):\n",
    "     print(i, values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Depending on your data-analytic goal, you may actually pursue outliers. For example, within healthcare data, an outlier might provide you with a genetic trait key to a cause or cure. Often, however, you will simply delete the outlier values. The following Python program, NoOutliers.py, again performs the mean and standard-deviation calculations, this time, however, with and without the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    " \n",
    "values = [-100, -75, 1,2,3,4,5, 75, 100]\n",
    "mean = statistics.mean(values)\n",
    "stdev = statistics.stdev(values)\n",
    "print('Starting values ', values)\n",
    "print('Mean ', mean)\n",
    "print('Standard deviation ', stdev) \n",
    "\n",
    "newvalues = []\n",
    "\n",
    "for i in range(len(values)):\n",
    "  if values[i] > (mean - stdev) and values[i] < (mean + stdev):\n",
    "     newvalues.append(values[i])\n",
    "\n",
    "mean = statistics.mean(newvalues)\n",
    "stdev = statistics.stdev(newvalues)\n",
    "\n",
    "print('\\nList without outliers ', newvalues)\n",
    "print('Mean ', mean)\n",
    "print('Standard deviation ', stdev) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Identifying Outliers Using DBSCAN\n",
    "\n",
    "When you cluster data sets, most cluster algorithms will assign all values to clusters, even outliers. As you have learned, the DBSCAN-clustering algorithm will identify “core” values, “border” values and noise. If a point is not in a cluster, meaning the point is noise (an outlier), the vector will contain the value -1. The following Python script, ShowNoise.py, displays the noise values identified by DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Chapter 10 (Python) / Deliverable 9\n",
    "######################################\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from pandas import DataFrame\n",
    "\n",
    "Data = {\n",
    "        'x': [35,34,32,37,33,33,31,27,35,34,62,54,57,47,50,57,59,52,61,47,50,48,39,40,45,47,39,44,50,48],\n",
    "        'y': [79,54,52,77,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,23,22,13,14,22,7,29,25,9,8]\n",
    "       }\n",
    "  \n",
    "df = DataFrame(Data,columns=['x','y'])\n",
    "\n",
    "clustering = DBSCAN(eps=5, min_samples=3).fit(df)\n",
    "labels = clustering.labels_\n",
    "\n",
    "i = 0\n",
    "print(\"Index\\tCluster\\t X\\t Y\")\n",
    "for row in df.iterrows():\n",
    "  if labels[i] == -1:\n",
    "    print(i,'     ', labels[i], '     ', row[1]['x'],'    ', row[1]['y'])\n",
    "  i = i + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
