{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Chapter 1: Data Mining and Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Some of the scripts presented in this notebook use several Python libraries which have been pre-installed for you. If you had been required to install these libaries on your own, you would issue the following commands:\n",
    "\n",
    "```python\n",
    "! pip install --user pandas\n",
    "! pip install --user matplotlib\n",
    "! pip install --user sklearn\n",
    "! pip install --user apriori\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Simple Visualization Example\n",
    "\n",
    "**Note: This notebook is intended to provide a demonstration of common data mining operations. As such, you are not expected to understand the entirety of the code in each sample.*\n",
    "\n",
    "Data analysts often leverage the Python and R Programming languages to perform data-mining and machine-learning operations. Data mining is the process of identifying patterns that exist within data, and one of the first steps data analysts perform to aid in pattern identification is to represent the data visually. Through transformation of datapoints into charts and graphs, the data can be parsed more easily, and patterns recognized more readily.\n",
    "\n",
    "Take the following Python script, TitanicCharts.py, for example. This script opens the Titanic dataset, which contains information about the passengers who lived and died on the Titanic, and uses this data to create three different pie charts showing the passenger assignments by class, survivors by class, and deaths by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 1 (Python) / Deliverable 1\n",
    "#####################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "Class = data[['Pclass']].values\n",
    "Survived = data[['Survived']].values\n",
    "\n",
    "FirstClass = 0\n",
    "SecondClass = 0 \n",
    "ThirdClass = 0\n",
    "FirstClassSurvived = 0\n",
    "FirstClassDied = 0\n",
    "SecondClassSurvived = 0 \n",
    "SecondClassDied = 0\n",
    "ThirdClassSurvived = 0 \n",
    "ThirdClassDied = 0\n",
    "\n",
    "for i in range(0, len(Class)):\n",
    "  if Class[i] == 1:\n",
    "    FirstClass += 1\n",
    "    if Survived[i] == 1: \n",
    "       FirstClassSurvived += 1\n",
    "    else:\n",
    "       FirstClassDied += 1\n",
    "\n",
    "  elif Class[i] == 2:\n",
    "    SecondClass += 1\n",
    "    if Survived[i] == 1: \n",
    "       SecondClassSurvived += 1\n",
    "    else:\n",
    "       SecondClassDied += 1\n",
    "\n",
    "  elif Class[i] == 3:\n",
    "    ThirdClass += 1\n",
    "    if Survived[i] == 1: \n",
    "       ThirdClassSurvived += 1\n",
    "    else:\n",
    "       ThirdClassDied += 1\n",
    "       \n",
    "\n",
    "# Data to plot\n",
    "labels = '1st Class', '2nd Class', '3rd Class'\n",
    "sizes = [FirstClass, SecondClass, ThirdClass]\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral']\n",
    "explode = (0.1, 0, 0)  # explode 1st slice\n",
    "plt.title(\"Passenger Class Assignment\")\n",
    "# Plot chart\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sizes = [FirstClassSurvived, SecondClassSurvived, ThirdClassSurvived]\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral']\n",
    "explode = (0.1, 0, 0)  # explode 1st slice\n",
    "plt.title(\"Surviving Passengers by Class Assignment\")\n",
    "# Plot chart\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sizes = [FirstClassDied, SecondClassDied, ThirdClassDied]\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral']\n",
    "explode = (0.1, 0, 0)  # explode 1st slice\n",
    "plt.title(\"Passenger Deaths by Class Assignment\")\n",
    "# Plot chart\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Predicting Titanic Deaths \n",
    "\n",
    "Data science, often used interchangeably with data mining, is the use of statistics, programming, scientific methods, and machine learning to extract knowledge from a dataset. Using patterns gleaned from data visualizations, data analysts can then train and test algorithms to produce useful models, some which may be leveraged to predict future outcomes with a high degree of accuracy.\n",
    "\n",
    "The following Python script, Titanic.py, opens the TitanicFields dataset which contains data about many of the passengers, such as age, gender, and class of travel. The dataset eliminates several of columns that are not used in the prediction, deletes records with missing data, and converts the text strings male and female to the values 1 and 0. The script uses random forest classification to predict, based on age, gender, and class of travel, whether a passenger would have lived or died:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 1 (Python) / Deliverable 2\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.tree as tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "names = ['Pclass','Sex','Age','Survived']\n",
    "\n",
    "df = pd.read_csv('TitanicFields.csv', header=None, names=names)\n",
    "X = np.array(df.iloc[:, 0:2]) \t\n",
    "\n",
    "y = np.array(df['Survived'])\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print ('Accuracy Score: ', accuracy_score(y_test, pred))\n",
    "print('\\nConfusion Matrix\\n', confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "Data clustering is the process of grouping related dataset items into one or more clusters (groups of related items based on shared features). Clustering uses an unsupervised machine-learning algorithm to generate such groups, which means the algorithm does not use a training dataset.\n",
    "\n",
    "Consider the Iris dataset, for example, a well-known data-mining and machine-learning dataset used to introduce the clustering and classification processes. The Iris dataset contains sepal and pedal lengths for three varieties of Iris flowers:\n",
    "\n",
    "    •\tIris-setosa\n",
    "    •\tIris-vergenica \n",
    "    •\tIris-versicolor\n",
    "\n",
    "The dataset has 50 records for each variety. Using clustering, you can collect the data into groups for further analysis. The following Python script, IrisCluster.py, uses the common k-Means clustering algorithm to identify related groups within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-02-27T15:57:53.654652Z",
     "iopub.status.busy": "2020-02-27T15:57:53.653652Z",
     "iopub.status.idle": "2020-02-27T15:57:55.141209Z",
     "shell.execute_reply": "2020-02-27T15:57:55.139213Z",
     "shell.execute_reply.started": "2020-02-27T15:57:53.654652Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 1 (Python) / Deliverable 3\n",
    "#####################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv('iris.data.csv', header=None, names=names)\n",
    "from pandas import DataFrame\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = df.drop('class', axis=1)\n",
    "pca = PCA(n_components=2).fit(df)\n",
    "data_2d = pca.transform(df)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3).fit(data_2d)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# plot number\n",
    "for i in range(0, data_2d.shape[0]):\n",
    "  if kmeans.labels_[i] == 0:\n",
    "    plt.scatter(data_2d[i,0], data_2d[i,1], c='green') \n",
    "  elif kmeans.labels_[i] == 1:\n",
    "    plt.scatter(data_2d[i,0], data_2d[i,1], c='yellow') \n",
    "  elif kmeans.labels_[i] == 2:\n",
    "    plt.scatter(data_2d[i,0], data_2d[i,1], c='blue')\n",
    "   \n",
    "\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X')\n",
    "plt.title(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Data Classification\n",
    "\n",
    "Data classification uses supervised machine learning, meaning the classification algorithm will use a training dataset to teach the algorithm the common attributes for each category. There are many different data-classification algorithms, which differ by memory use and CPU performance.\n",
    "\n",
    "Consider, for example, the Census dataset at the University of California Irvine (UCI) that contains census data on individuals such as age, gender, race, and marital status. The following program uses a subset of the dataset which has reduced the number of records to 500 and reduced the number of columns. The dataset has also converted text fields, such as male and female (to the numeric values 1 and 0), and marital status (0 not married, 1 married, 2 never married, 3 divorced, and 4 widowed). The dataset represents individuals living in the United States as 1 and outside of the United States as 0. The dataset represents incomes lower than 50,000 with the numeric value 0 and incomes more than 50,000 as 1. \n",
    "\n",
    "The following Python program, PredictIncome.py, uses the census dataset to classify an individual as likely to fall within one of two income levels:\n",
    "\n",
    "    •\tEarns less than 50,000\n",
    "    •\tEarns more than 50,000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-02-27T15:58:17.590919Z",
     "iopub.status.busy": "2020-02-27T15:58:17.590919Z",
     "iopub.status.idle": "2020-02-27T15:58:17.647919Z",
     "shell.execute_reply": "2020-02-27T15:58:17.646919Z",
     "shell.execute_reply.started": "2020-02-27T15:58:17.590919Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 1 (Python) / Deliverable 4\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "names = ['Married', 'Race', 'Gender', 'Age', 'Country', 'Income']\n",
    "\n",
    "df = pd.read_csv('census.csv', header=1, names=names)\n",
    "X = np.array(df.iloc[:, 0:4]) # select the (range of) attributes to base the classification on.\n",
    "\n",
    "y = np.array(df['Income'])\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "print ('\\nModel accuracy score: ', accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Predicting Patient Healthcare Costs\n",
    "\n",
    "For decades, business have used data analytics to explain their performance during the previous quarter or year. Analysts refer to this as descriptive analytics-the analyts use data to describe what happened in the past. Predictive analytics, in contrast, uses data to predict what will happen in the future.\n",
    "\n",
    "Consider the insurance.csv dataset, which contains over 1,300 records regarding insurance customers and their insurance charges. The following Python script, PredictInsuranceCosts.py, uses the dataset to create a predictive model that analysts can then use to predict a customer’s healthcare costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-02-27T16:00:13.705738Z",
     "iopub.status.busy": "2020-02-27T16:00:13.704739Z",
     "iopub.status.idle": "2020-02-27T16:00:15.566157Z",
     "shell.execute_reply": "2020-02-27T16:00:15.566157Z",
     "shell.execute_reply.started": "2020-02-27T16:00:13.704739Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 1 (Python) / Deliverable 5\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "data = pd.read_csv('insurance.csv')\n",
    "\n",
    "X = data[['age', 'sex', 'bmi', 'children', 'smoker', 'region']].values # select the values to base the prediction on\n",
    "y = data['charges']\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(model.feature_importances_)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "for index in range(len(predictions)):\n",
    "  print('Actual: ', y[index], 'Predicted: ', predictions[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Using Association with E-Commerce Data\n",
    "\n",
    "Data association is the process of identifying relationships between variables, for which the presence or absence of a first variable (called the antecedent) influences a second variable (called the consequent). One of the best-known data-association problems is market-basket analysis, which examines the items in a shopper’s basket to identify associations between them. Using market-basket analysis, for example, analysts found that shoppers who purchased diapers, are highly likely to also purchase beer. With such product insights in hand, a store might advertise a sale on diapers, while also increasing the price of beer. Or, the store may place beer and diapers far away from one another, so that customers needing both must walk past many other items.\n",
    "\n",
    "Assume, for example, a popular e-commerce site wants to know how other product sales are driven by their key items: books, videos, and music. The dataset for the e-commerce sales contains transactions. Each transaction lists items purchased. The following Python script, EcommerceAssociation.py, uses the apriori algorithm to determine the associations between products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Chapter 1 (Python) / Deliverable 6\n",
    "#####################################\n",
    "\n",
    "import pandas as pd\n",
    "from apyori import apriori\n",
    "\n",
    "data = pd.read_csv('ecommerce.csv', header=None)\n",
    "\n",
    "records = []\n",
    "for i in range(0, len(data)):  \n",
    "    records.append([str(data.values[i,j]) for j in range(0, len(data.columns))])\n",
    "#1.1\n",
    "rules = apriori(records, min_length=2, min_lift=1.1, min_support=0.15)  \n",
    "results = list(rules)  \n",
    "\n",
    "for item in results:\n",
    "  if not 'nan' in str(item):\n",
    "   print()\n",
    "   print(item)\n",
    "   print()\n",
    "   print(\"-----------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "record_timing": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
